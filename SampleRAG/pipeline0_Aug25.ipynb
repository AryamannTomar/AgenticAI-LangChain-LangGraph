{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77563a41",
   "metadata": {},
   "source": [
    "##### IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "79be8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tqdm \n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_cerebras import ChatCerebras\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma, FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    AIMessage,\n",
    "    trim_messages\n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ['HF_TOKEN'] = os.getenv(\"HF_TOKEN\")\n",
    "os.environ['GROQ_API_KEY'] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ['CEREBRAS_API_KEY'] = os.getenv(\"CEREBRAS_API_KEY\")\n",
    "os.environ['ENDPOINT_URL'] = os.getenv(\"ENDPOINT_URL\")\n",
    "os.environ['AZURE_OPENAI_API_KEY'] = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "os.environ['AZURE_OPENAI_API_VERSION'] = os.getenv(\"AZURE_OPENAI_API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "730d0e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = [\n",
    "    \"Current revenue model - Fixed Fee/Time & Material Contract/Volume based Invoicing (Please select multiple option if required)\",\n",
    "    \"Are we required to share FTE details at the time of invoicing?\",\n",
    "    \"Does GEP have a negotiated rate card with the client?\",\n",
    "    \"Have we built any year-over-year efficiencies into the solution/SOW? If yes, what are the committed efficiency targets?\",\n",
    "    \"What are the start and end dates of this SOW\",\n",
    "    \"what is the renewal clause\",\n",
    "    \"Is there Termination for convenience\",\n",
    "    \"Is travel billable or not\",\n",
    "    \"what are the major SLA's\",\n",
    "    \"is there clause for fees at risk\",\n",
    "    \"What are the payment terms\",\n",
    "    \"is there Clause for COLA\",\n",
    "    \"what is the category of work we are doing\",\n",
    "    \"What is the credit period of the contract\",\n",
    "    \"what is the invoicing schedule\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ddbcd6a",
   "metadata": {},
   "source": [
    "##### INGESTION - SPLITTING - EMBEDDINGS - PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intfloat/e5-large-v2 | BAAI/bge-base-en-v1.5 | Qwen/Qwen3-Embedding-0.6B\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\") \n",
    "# vectordb=Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "model_name = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21527356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'P1' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR='./FW__Contracts'\n",
    "doc_list=os.listdir(BASE_DIR)\n",
    "doc0_name=doc_list[0]\n",
    "doc0=PDFPlumberLoader(BASE_DIR+'/'+doc0_name).load()\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "f_docs=text_splitter.split_documents(doc0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a537c5f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m vectordb=\u001b[43mChroma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mf_docs\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./chroma_db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:887\u001b[39m, in \u001b[36mChroma.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    885\u001b[39m texts = [doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m    886\u001b[39m metadatas = [doc.metadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m--> \u001b[39m\u001b[32m887\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:843\u001b[39m, in \u001b[36mChroma.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[39m\n\u001b[32m    835\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_batches\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m create_batches(\n\u001b[32m    838\u001b[39m         api=chroma_collection._client,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    839\u001b[39m         ids=ids,\n\u001b[32m    840\u001b[39m         metadatas=metadatas,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    841\u001b[39m         documents=texts,\n\u001b[32m    842\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m         \u001b[43mchroma_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    849\u001b[39m     chroma_collection.add_texts(texts=texts, metadatas=metadatas, ids=ids)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\vectorstores\\chroma.py:277\u001b[39m, in \u001b[36mChroma.add_texts\u001b[39m\u001b[34m(self, texts, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m texts = \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embedding_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[32m    279\u001b[39m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[32m    280\u001b[39m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[32m    281\u001b[39m     length_diff = \u001b[38;5;28mlen\u001b[39m(texts) - \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:115\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    113\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings.tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:752\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    750\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    751\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m output_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m output_tokens = output_states[\u001b[32m0\u001b[39m]\n\u001b[32m    445\u001b[39m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\utils\\generic.py:965\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    962\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    967\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:576\u001b[39m, in \u001b[36mQwen3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    565\u001b[39m         partial(decoder_layer.\u001b[34m__call__\u001b[39m, **flash_attn_kwargs),\n\u001b[32m    566\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    573\u001b[39m         position_embeddings,\n\u001b[32m    574\u001b[39m     )\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    588\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:304\u001b[39m, in \u001b[36mQwen3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n\u001b[32m    303\u001b[39m residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpost_attention_layernorm\u001b[49m(hidden_states)\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    306\u001b[39m hidden_states = residual + hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Aryam\\anaconda3\\envs\\llms\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1915\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# It is crucial that the return type is not annotated as `Any`, otherwise type checking\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[39;00m\n\u001b[32m   1914\u001b[39m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/115074\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1915\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Union[Tensor, \u001b[33m\"\u001b[39m\u001b[33mModule\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1916\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1917\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "vectordb=Chroma.from_documents(documents=f_docs,embedding=hf, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750b045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = ChatPromptTemplate.from_template(\n",
    "#     \"\"\"\n",
    "#     Answer the following question strictly based on the provided context. Keep the response highly brief, short, and crisp.:\n",
    "#     <context>\n",
    "#     {context}\n",
    "#     </context>\n",
    "#     \"\"\")\n",
    "\n",
    "# #* OUTPUT FORMAT in PD DataFrame\n",
    "# results = []\n",
    "# for i in tqdm.tqdm(range(len(sample_questions))):\n",
    "#     query = sample_questions[i]\n",
    "#     response = retrieval_chain.invoke({\"input\": query})\n",
    "#     results.append([query, response['answer']])\n",
    "# df = pd.DataFrame(results, columns=[\"Question\", \"Answer\"])\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "#     display(df.style.set_properties(**{'text-align': 'left'}).set_table_styles(\n",
    "#         [{'selector': 'th', 'props': [('text-align', 'left')]}]\n",
    "#     ))\n",
    "\n",
    "#############################################################################################################################################################\n",
    "# Gemma2-9b-It\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", groq_api_key=os.environ['GROQ_API_KEY'])\n",
    "# llm = ChatCerebras(model=\"gpt-oss-120b\", cerebras_api_key=os.environ['CEREBRAS_API_KEY'])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Answer the following question strictly based on the provided context.\n",
    "     Keep the response highly brief, short, and crisp.:\n",
    "     <context>\n",
    "     {context}\n",
    "     </context>\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(vectordb.as_retriever(), create_stuff_documents_chain(llm, prompt))\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "trimmer = trim_messages(\n",
    "    max_tokens=45,\n",
    "    strategy=\"last\",\n",
    "    token_counter=llm,\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | retrieval_chain\n",
    "    | (lambda x: {\"output\": x[\"answer\"], **x})\n",
    ")\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"test_rag1\"}}\n",
    "query = \"which are the 2 parties between which the contract is signed?\"\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"input\": query,\n",
    "    },\n",
    "    config=config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ace8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The contract is between WestRock and the Supplier.\n"
     ]
    }
   ],
   "source": [
    "llm = ChatCerebras(\n",
    "    model=\"gpt-oss-120b\",  # | \"llama3.1-8b\",\n",
    "    cerebras_api_key=os.environ['CEREBRAS_API_KEY']\n",
    ")\n",
    "\n",
    "# Your existing prompt and retrieval chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"\"\"Answer the following question strictly based on the provided context.\n",
    "     Keep the response highly brief, short, and crisp.:\n",
    "     <context>\n",
    "     {context}\n",
    "     </context>\"\"\"\n",
    "    ),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "])\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "    vectordb.as_retriever(), \n",
    "    create_stuff_documents_chain(llm, prompt)\n",
    ")\n",
    "\n",
    "# Memory store\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
    "    | retrieval_chain\n",
    "    | (lambda x: {\"output\": x[\"answer\"], **x})\n",
    ")\n",
    "\n",
    "# Create chain with memory\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\"\n",
    ")\n",
    "\n",
    "# Usage\n",
    "config = {\"configurable\": {\"session_id\": \"test_rag1\"}}\n",
    "query = \"which are the 2 parties between which the contract is signed?\"\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=query)],\n",
    "        \"input\": query,\n",
    "    },\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa39d42",
   "metadata": {},
   "source": [
    "##### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f1609f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:11<00:00,  1.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d2ac6 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_d2ac6_row0_col0, #T_d2ac6_row0_col1, #T_d2ac6_row1_col0, #T_d2ac6_row1_col1, #T_d2ac6_row2_col0, #T_d2ac6_row2_col1, #T_d2ac6_row3_col0, #T_d2ac6_row3_col1, #T_d2ac6_row4_col0, #T_d2ac6_row4_col1, #T_d2ac6_row5_col0, #T_d2ac6_row5_col1, #T_d2ac6_row6_col0, #T_d2ac6_row6_col1, #T_d2ac6_row7_col0, #T_d2ac6_row7_col1, #T_d2ac6_row8_col0, #T_d2ac6_row8_col1, #T_d2ac6_row9_col0, #T_d2ac6_row9_col1, #T_d2ac6_row10_col0, #T_d2ac6_row10_col1, #T_d2ac6_row11_col0, #T_d2ac6_row11_col1, #T_d2ac6_row12_col0, #T_d2ac6_row12_col1, #T_d2ac6_row13_col0, #T_d2ac6_row13_col1, #T_d2ac6_row14_col0, #T_d2ac6_row14_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d2ac6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d2ac6_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
       "      <th id=\"T_d2ac6_level0_col1\" class=\"col_heading level0 col1\" >Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d2ac6_row0_col0\" class=\"data row0 col0\" >Current revenue model - Fixed Fee/Time & Material Contract/Volume based Invoicing (Please select multiple option if required)</td>\n",
       "      <td id=\"T_d2ac6_row0_col1\" class=\"data row0 col1\" >- Fixed Fee  \n",
       "- Volume‑based Invoicing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d2ac6_row1_col0\" class=\"data row1 col0\" >Are we required to share FTE details at the time of invoicing?</td>\n",
       "      <td id=\"T_d2ac6_row1_col1\" class=\"data row1 col1\" >No. The provided documents do not require sharing FTE details when invoicing.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d2ac6_row2_col0\" class=\"data row2 col0\" >Does GEP have a negotiated rate card with the client?</td>\n",
       "      <td id=\"T_d2ac6_row2_col1\" class=\"data row2 col1\" >The provided documents do not mention a negotiated rate card between GEP and the client.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d2ac6_row3_col0\" class=\"data row3 col0\" >Have we built any year-over-year efficiencies into the solution/SOW? If yes, what are the committed efficiency targets?</td>\n",
       "      <td id=\"T_d2ac6_row3_col1\" class=\"data row3 col1\" >The excerpt does not specify any year‑over‑year efficiency targets. It describes the types of savings (soft, tangible, working‑capital) and the activities needed to realize them, but it does not include concrete YoY efficiency commitments.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d2ac6_row4_col0\" class=\"data row4 col0\" >What are the start and end dates of this SOW</td>\n",
       "      <td id=\"T_d2ac6_row4_col1\" class=\"data row4 col1\" >**Start date:** May 15 2023  \n",
       "**End date:** February 28 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_d2ac6_row5_col0\" class=\"data row5 col0\" >what is the renewal clause</td>\n",
       "      <td id=\"T_d2ac6_row5_col1\" class=\"data row5 col1\" >The provided excerpt does not contain any renewal clause.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_d2ac6_row6_col0\" class=\"data row6 col0\" >Is there Termination for convenience</td>\n",
       "      <td id=\"T_d2ac6_row6_col1\" class=\"data row6 col1\" >Yes. The document refers to termination “for convenience” (e.g., “if the Supplement were terminated for convenience…”).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_d2ac6_row7_col0\" class=\"data row7 col0\" >Is travel billable or not</td>\n",
       "      <td id=\"T_d2ac6_row7_col1\" class=\"data row7 col1\" >The provided context does not contain any information about travel being billable or non‑billable.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_d2ac6_row8_col0\" class=\"data row8 col0\" >what are the major SLA's</td>\n",
       "      <td id=\"T_d2ac6_row8_col1\" class=\"data row8 col1\" >The key Service Level (SLA) categories in the document are:\n",
       "\n",
       "| Type | Workstream | Service‑Level Title |\n",
       "|------|-----------|----------------------|\n",
       "| **CSL 1** | S2C | Cumulative Realized Hard Savings |\n",
       "| **CSL 2** | P2P | PO Creation Timeliness |\n",
       "| **CSL 3** | S2P | WestRock Satisfaction Survey |\n",
       "| **CSL 4** | P2P | Helpdesk Ticket Resolution |\n",
       "| **CSL 5** | Technology | System Uptime |\n",
       "\n",
       "In addition, the contract defines ten KPIs (KPI 1‑10) that are also measured, covering procurement audit compliance, spend management, vendor diversity, contract spend, spot‑buy efficiency, PO acknowledgment, soft savings, catalog pricing completeness, and blocked‑invoice percentage.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_d2ac6_row9_col0\" class=\"data row9 col0\" >is there clause for fees at risk</td>\n",
       "      <td id=\"T_d2ac6_row9_col1\" class=\"data row9 col1\" >Yes. Clause 2.27 defines “Fees at Risk,” referring to its meaning in Schedule A‑3, and Schedule A‑3 outlines true‑up procedures for Variable Fees and Fees at Risk.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_d2ac6_row10_col0\" class=\"data row10 col0\" >What are the payment terms</td>\n",
       "      <td id=\"T_d2ac6_row10_col1\" class=\"data row10 col1\" >Net 120 day payment terms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_d2ac6_row11_col0\" class=\"data row11 col0\" >is there Clause for COLA</td>\n",
       "      <td id=\"T_d2ac6_row11_col1\" class=\"data row11 col1\" >Yes. The COLA provisions are outlined in Clause 8 of Schedule A‑4 (Cost of Living Adjustment).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_d2ac6_row12_col0\" class=\"data row12 col0\" >what is the category of work we are doing</td>\n",
       "      <td id=\"T_d2ac6_row12_col1\" class=\"data row12 col1\" >Category Management and procurement transformation (source‑to‑contract & procure‑to‑pay) services.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_d2ac6_row13_col0\" class=\"data row13 col0\" >What is the credit period of the contract</td>\n",
       "      <td id=\"T_d2ac6_row13_col1\" class=\"data row13 col1\" >The contract provides a 120‑day credit period (net 120 days).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d2ac6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_d2ac6_row14_col0\" class=\"data row14 col0\" >what is the invoicing schedule</td>\n",
       "      <td id=\"T_d2ac6_row14_col1\" class=\"data row14 col1\" >Invoices are issued monthly. Termination charges are added to the regular monthly invoice for the final month of service; if any Affected Services continue after the termination date, the supplier invoices each subsequent month for a portion of the termination charge (A × B⁄C) until the total amount is paid.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2270d6915d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "config = {\"configurable\": {\"session_id\": \"batch_test\"}}\n",
    "\n",
    "for query in tqdm.tqdm(sample_questions):\n",
    "    response = with_message_history.invoke(\n",
    "        {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"input\": query,\n",
    "        },\n",
    "        config=config,\n",
    "    )\n",
    "    results.append([query, response[\"output\"]])  \n",
    "    \n",
    "df = pd.DataFrame(results, columns=[\"Question\", \"Answer\"])\n",
    "with pd.option_context('display.max_rows', None, 'display.max_colwidth', None):\n",
    "    display(\n",
    "        df.style.set_properties(**{'text-align': 'left'})\n",
    "        .set_table_styles([{'selector': 'th', 'props': [('text-align', 'left')]}])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389077d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
